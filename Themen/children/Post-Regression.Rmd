

name: teil1
class: center, middle, inverse


```{r Post-Regression-1}
library("latex2exp")
library("patchwork")
library("gt")
library("rstanarm")
library("tidyverse")
```



# Post-Verteilung der Regression

---


## Einfache Regression


.pull-left[

- Die (einfache) Regression prüft, inwieweit zwei Variablen, $Y$ und $X$ linear zusammenhängen.
- Je mehr sie zusammenhängen, desto besser kann man $X$ nutzen, um $Y$ vorherzusagen (und umgekehrt).
- Hängen $X$ und $Y$ zusammen, heißt das nicht (unbedingt), dass es einen *kausalen* Zusammenhang zwische $X$ und $Y$ gibt.
- Linear bedeutet, der Zusammenhang ist additiv und konstant: wenn $X$ um eine Einheit steigt, steigt $Y$ immer um $b$ Einheiten.

]

.pull-right[

```{r Post-Regression-2, fig.asp = 1}
library(tidyverse)

Kung_path <- "https://tinyurl.com/jr7ckxxj"  # Datenquelle s.o.

d <- read_csv(Kung_path)  

d2 <- 
  d %>% 
  filter(age > 18)

d2 %>% 
  #select(weight, height) %>% 
  #drop_na() %>% 
  ggplot(
       aes(x = weight, y = height)) +
  geom_point(alpha = .7) +
  geom_smooth(method = "lm")

```


]

---

## Statistiken zum !Kung-Datensatz

[Datenquelle](https://raw.githubusercontent.com/sebastiansauer/2021-wise/main/Data/Howell1a.csv)

.tiny[
```{r Post-Regression-3, echo = TRUE, eval = FALSE}
library(tidyverse)
library(rstatix)
Kung_path <- "https://tinyurl.com/jr7ckxxj"  # Datenquelle s.o.

d <- read_csv(Kung_path)  

d2 <- 
  d %>% 
  filter(age > 18)

get_summary_stats(d2)
```

]


```{r Post-Regression-4, echo = FALSE, eval = TRUE}
library(rstatix)

get_summary_stats(d2) %>% 
  gt() %>% 
  fmt_number(columns = 2:13, decimals = 1)
```



```{r Post-Regression-5}
weight_mean <- mean(d2$weight)
weight_sd <- sd(d2$height)

height_mean <- mean(d2$height)
height_sd <- sd(d2$height)

d2_summary <-
  d2 %>% 
  select(weight, height) %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  summarise(avg = mean(value),
            stdev = sd(value))
```

Das mittlere Körpergewicht (`weight`) liegt bei ca. 45kg (sd 7 kg).

---

## Visualisierung von `weight` und `height`

Explorative Datenanalyse (keine Inferenz auf Populationswerte, sondern auf die Stichprobe bezogen)

```{r Post-Regression-6, echo = FALSE, fig.asp=.5}
d2 %>% 
  select(weight, height) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free") +
  geom_vline(data = d2_summary,
             aes(xintercept = avg)) +
  geom_segment(data = d2_summary,
              aes(x = avg-stdev,
                  xend = avg+stdev),
              y = 0,
              yend = 0,
              alpha = .7,
              color = "blue",
              size = 2) +
  labs(caption = "Vertikale Linie: Mittelwert\nhorizontale Linie: Std. Abweichung")
```


---



## Prädiktor zentrieren 1/2 



- Zieht man von jedem Gewichtswert den Mittelwert ab, so bekommt man die Abweichung des Gewichts vom Mittelwert (Präditkor "zentrieren").
- Wenn man den Prädiktor (`weight`) zentriert hat, ist der Achsenabschnitt, $\alpha$, einfacher zu verstehen.
- In einem Modell mit zentriertem Prädiktor (`weight`) gibt der Achsenabschnitt die Größe einer Person mit durchschnittlichem Gewicht an. 
- Würde man `weight` nicht zentrieren, gibt der Achsenabschnitt die Größe einer Person mit `weight=0` an, was nicht wirklich sinnvoll zu interpretieren ist.

---



## Prädiktor zentrieren 2/2



.pull-left[
```{r Post-Regression-7, echo = TRUE}
d2 <-
  d2 %>% 
  mutate(
    weight_c = weight - 
      mean(weight))
```

]

.pull-right[
```{r Post-Regression-8}
d2 %>% 
  slice_head(n=3) %>% 
  gt() %>% 
  fmt_number(columns = everything(), decimals = 0)
```

]


```{r Post-Regression-9, fig.asp=0.5}
d2 %>% 
  select(weight, weight_c) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free")
```


---


## Für jede Ausprägung des Prädiktors brauchen wir eine  Post-Verteilung für $\mu$



```{r m42-read-from-disk, echo = FALSE}

m42 <- read_rds("objects/m42.rds")

m42_post <- as_tibble(m42)
names(m42_post) <- c("mu", "sigma")

```




```{r Post-Regression-10}
plot_post_42 <- 
  m42_post %>% 
  ggplot() +
  aes(x = mu) +
  geom_density(fill = "grey60") +
  labs(x = expression(mu),
       title = TeX("Posteriori-Verteilung für $\\mu$, m42")) +
  scale_y_continuous(breaks = NULL)
```

```{r Post-Regression-11}

lm1 <- lm(height ~ weight, data = d2)

d_pred <-
  tibble(weight = c(40, 45, 50, 55),
         height = predict(lm1, newdata = data.frame(weight)))

plot_condition <- 
  d2 %>% 
  #select(weight, height) %>% 
  #drop_na() %>% 
  ggplot(
       aes(x = weight, y = height)) +
  geom_point(alpha = .7) +
  geom_smooth(method = "lm") +
  geom_point(data = d_pred, color = "red", size = 5, alpha = .5) +
  labs(title = "Für jeden Wert von X\nwird eine Post-Vert. berechnet")
```

```{r m3}
m43 <- 
  stan_glm(height ~ weight_c, 
           prior = normal(0, 10),
           prior_intercept = normal(178, 20),  # mu
           prior_aux = exponential(0.1),  # sigma
           refresh = FALSE,  # bitte nicht so viel Ausgabe drucken
           data = d2)

m43_prior_pred <-
    stan_glm(height ~ weight_c, 
           prior_intercept = normal(178, 20),  # mu
           prior_aux = exponential(0.1),  # sigma
           refresh = FALSE, 
           prior_PD = TRUE,
           data = d2)
```

```{r eval = FALSE}
print(m43)
summary(m43_prior_pred)
write_rds(m43, "objects/m43.rds")
```




```{r m43-plot}

nd <- tibble(
  weight_c = c(-5, 0, +5, 10)
)

ppv_m43 <- 
  posterior_predict(m43, , newdata = nd) %>% 
  as_tibble() %>% 
  pivot_longer(everything(), 
               names_to = "weight_condition",
               values_to = "h")



ppv_m43_summary <-
  ppv_m43 %>% 
  group_by(weight_condition) %>% 
  summarise(m = mean(h),
            s = sd(h))


post_at_plot <-
  ppv_m43 %>% 
  ggplot() +
  aes(x = h) +
  geom_density(fill = "grey60") +
  facet_wrap(~ weight_condition, nrow = 1, scales = "free") +
  scale_y_continuous(breaks = NULL) +
  labs(title = "Post-Verteilungen an verschiedenen Werten von X",
       caption = "MW±2sd") +
  geom_point(data = ppv_m43_summary,
             aes(x = m,
                 y = 0),
             size = 2, color = "blue", alpha = .5) +
  geom_segment(data = ppv_m43_summary,
               aes(x = m-2*s,
                   xend = m+2*s),
               y = 0,
               yend = 0,
               color = "blue",
               alpha = .5,
               size = 2)
```



```{r Post-Regression-12, out.width="100%", fig.width=7}
p1 <- (plot_post_41 | plot_condition) / post_at_plot
p1
```

---

## Modelldefinition von `m43`

- Für jede Ausprägung von `weight`, $h_i$ wird eine Post-Verteilung für `height` berechnet.
- Der Mittelwert $\mu$ für jede Post-Verteilung ergibt sich aus dem .green[linearen Modell (unserer Regressionsformel)].
- Die Post-Verteilung berechnet sich auf Basis der .blue[Priori-Werte] und des .red[Likelihood] (Bayes-Formel).
- Wir brauchen .blue[Priori-Werte] für die Steigung $\beta$ und den Achsenabschnitt $\alpha$ der Regressionsgeraden.
- Außerdem brauchen wir einen .blue[Priori-Wert], der die Streuung $\sigma$ der Größe (`height`) angibt; dieser Wert wird als exonentialverteilt angenommen.
- Der .green[Likelihood] gibt an, wie wahrscheinlich ein Wert `height` ist, gegeben $\mu$ und $\sigma$.


$$
\begin{align*}
\color{red}{\text{height}_i} & \color{red}\sim \color{red}{\operatorname{Normal}(\mu_i, \sigma)} && \color{red}{\text{Likelihood}} \\
\color{green}{\mu_i} & \color{green}= \color{green}{\alpha + \beta\cdot \text{weight}_i}  && \color{green}{\text{Lineares Modell} } \\
\color{blue}\alpha & \color{blue}\sim \color{blue}{\operatorname{Normal}(178, 20)} && \color{blue}{\text{Priori}} \\
\color{blue}\beta  & \color{blue}\sim \color{blue}{\operatorname{Normal}(0, 10)}  && \color{blue}{\text{Priori}}\\
\color{blue}\sigma & \color{blue}\sim \color{blue}{\operatorname{Exp}(0.1)}  && \color{blue}{\text{Priori}}
\end{align*}
$$


---

## Likelihood, `m43`



$$
\begin{aligned}
\color{red}{\text{height}_i} & \color{red}\sim \color{red}{\operatorname{Normal}(\mu_i, \sigma)} && \color{red}{\text{Likelihood}}
\end{aligned}
$$




- Der Likelihood von `m43` ist ähnlich zu den vorherigen Modellen (`m41, m42`).
- Nur gibt es jetzt ein kleines "Index-i" am $\mu$ und am $h$ (h wie `heights`).
- Es gibt jetzt nicht mehr nur einen Mittelwert $\mu$, sondern für jede Beobachtung (Zeile) einen Mittelwert $\mu_i$.
- Lies  etwa so:

>    "Die Wahrscheinlichkeit, eine bestimmte Größe bei Person $i$ zu beobachten, gegeben $\mu$ und $\sigma$ ist normalverteilt (mit Mittelwert $\mu$ und Streuung $\sigma$)".



---

## Regressionsformel, `m43`


$$
\begin{aligned}
\color{green}{\mu_i} & \color{green}= \color{green}{\alpha + \beta\cdot \text{weight}_i}  && \color{green}{\text{Lineares Modell} } \\
\end{aligned}
$$


- $\mu$ ist jetzt nicht mehr ein Parameter, der (stochastisch) geschätzt werden muss. $\mu$ wird jetzt (deterministisch) *berechnet*. Gegeben $\alpha$ und $\beta$ ist $\mu$ ohne Ungewissheit bekannt.
- $\text{weight}_i$ ist der Prädiktorwert (`weight`) der $i$ten Beobachtung, also einer !Kung-Person (Zeile $i$ im Datensatz).
- Lies  etwa so:

>    "Der Mittelwert $\mu_i$ der $i$ten Person berechnet sich als Summe von $\alpha$ und $\beta \cdot \text{weight}_i$".


- $\mu_i$ ist eine lineare Funktion von `weight`.
- $\beta$ gibt den Unterschied in `height` zweier Beobachtung an, die sich um eine Einheit in `weight` unterscheiden (Steigung der Regressionsgeraden).
- $\alpha$ gibt an, wie groß $\mu$ ist, wenn `weight` Null ist.

---


## Priori-Werte der Regression, `m43`



$$
\begin{align*}
\color{blue}\alpha & \color{blue}\sim \color{blue}{\operatorname{Normal}(178, 20)} && \color{blue}{\text{Priori}} \\
\color{blue}\beta  & \color{blue}\sim \color{blue}{\operatorname{Normal}(0, 10)}  && \color{blue}{\text{Priori}}\\
\color{blue}\sigma & \color{blue}\sim \color{blue}{\operatorname{Exp}(0.1)}  && \color{blue}{\text{Priori}}
\end{align*}
$$

- Parameter sind hypothetische Kreaturen: Man kann sie nicht beobachten, sie existieren nicht wirklich. Ihre Verteilungen nennt man Priori-Verteilungen.
- $\alpha$ wurde in `m41` als $\mu$ bezeichnet, da wir dort eine "Regression ohne Prädiktoren" berechnet haben.
- $\sigma$ ist uns schon als Parameter bekannt und behält seine Bedeutung.
- $\beta$ fasst unser Vorwissen, ob und wie sehr der Zusammenhang zwischen Gewicht und Größe positiv (gleichsinnig ist).
   - 🤔 Moment. Dieser Prior, $\beta$ erachtet positive und negative Zusammenhang als gleich wahrscheinlich?!
    
    
---


## Prior-Prädiktiv-Simulation für `m43`


.right-column[

```{r echo = TRUE}
m43_prior_pred <-
    stan_glm(height ~ weight_c, 
             prior = normal(0, 10),
             prior_intercept = normal(178, 20),  # mu
             prior_aux = exponential(0.1),  # sigma
             refresh = FALSE, 
             prior_PD = TRUE,  # DIESER Schalter machts
             data = d2)


m43_prior_pred_draws <- 
  m43_prior_pred %>% 
  as_tibble() %>% 
  rename(a = `(Intercept)`) 
```

]


.left-column[

```{r}
m43_prior_pred_draws %>% 
  slice_head(n=5) %>% 
  gt() %>% 
  fmt_number(everything(), decimals = 1)
```

]


---

## Visualisieren der Prior-Prädiktiv-Verteilung

.pull-left[

```{r prior-pv1, echo = TRUE, eval = FALSE}
d2 %>% 
  ggplot(aes(x = weight_c,
             y = height)) +
  geom_point() +
  geom_abline(data = m43_prior_pred_draws %>% slice_head(n = 50),
              aes(intercept = a,
                  slope = weight_c),
              color = "skyblue", size = 0.2) +
  scale_y_continuous(limits = c(0, 500)) +
  geom_hline(yintercept = 272, size = .5) +
  geom_hline(yintercept = 0, linetype = "dashed")
```


]


.pull-right[
🤯 Oh nein! Einige dieser Regressionsgeraden sind unsinnig!


```{r ref.label = "prior-pv1", eval = TRUE}

```


.tiny[Die durchgezogene horizontale Linie gibt die Größe des [größten Menschens, Robert Pershing Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow), an.]

]


---


## Ein positiver Wert für $\beta$ ist plausibler


.pull-left[
### Oh no

Eine Normalverteilung mit viel Streuung:

```{r Post-Regression-16}
d <-
  tibble(
    x = seq(-30,30,.1),
    y = dnorm(x, mean = 0, sd = 10)
  )

d %>% 
  ggplot(aes(x,y)) +
  geom_line() +
  scale_y_continuous(breaks = NULL) +
  labs(title = "mu=0, s=10")
```

🤯 $\beta=-20$ wäre mit diesem Prior gut möglich: Pro kg Gewicht sind Menschen im Schnitt 20cm kleiner, laut dem Modell. Quatsch.
]


.pull-right[
### Oh yes

Wir bräuchten eher so eine Verteilung:

```{r Post-Regression-17}
d <-
  tibble(
    x = seq(-30,30,.1),
    y = dnorm(x, mean = 5, sd = 3)
  )

d %>% 
  ggplot(aes(x,y)) +
  geom_line() +
  scale_y_continuous(breaks = NULL) +
  labs(title = "mu=5, sd = 3")
```

Vermutlich besser: Ein Großteil der Wahrscheinlichkeitsmasse ist $X>0$. Allerdings gibt's keine Gewähr, dass unser Prior "richtig" ist.
]




---

## Priori-Prädiktiv-Simulation, 2. Versuch




.right-column[

```{r echo = TRUE}
m43a_prior_pred <-
    stan_glm(height ~ weight_c, 
             prior = normal(3, 2),
             prior_intercept = normal(178, 20),  # mu
             prior_aux = exponential(0.1),  # sigma
             refresh = FALSE, 
             prior_PD = TRUE,  # DIESER Schalter machts
             data = d2)


m43a_prior_pred_draws <- 
  m43a_prior_pred %>% 
  as_tibble() %>% 
  rename(a = `(Intercept)`) 
```

]


.left-column[

```{r}
m43a_prior_pred_draws %>% 
  slice_head(n=5) %>% 
  gt() %>% 
  fmt_number(everything(), decimals = 1)
```

]

---


## Visualisieren der Prior-Prädiktiv-Verteilung, `m43a`

.pull-left[

```{r prior-pv2, echo = TRUE, eval = FALSE}
d2 %>% 
  ggplot(aes(x = weight_c,
             y = height)) +
  geom_point() +
  geom_abline(data = m43a_prior_pred_draws %>% slice_head(n = 50),
              aes(intercept = a,
                  slope = weight_c),
              color = "skyblue", size = 0.2) +
  scale_y_continuous(limits = c(0, 500)) +
  geom_hline(yintercept = 272, size = .5) +
  geom_hline(yintercept = 0, linetype = "dashed")
```


]


.pull-right[

```{r ref.label = "prior-pv2", eval = TRUE}

```


.tiny[Die durchgezogene horizontale Linie gibt die Größe des [größten Menschens, Robert Pershing Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow), an.]

]


Das sieht gut aus; unsere Priori-Werte scheinen vernünftige Vorhersagen zu tätigen.

---



## Prior-Prädiktiv mit R plotten: Ausgabe

```{r Post-Regression-24, reeval = TRUE, echo = FALSE}
prior_pred_plot
```




---

## Moment, kann hier jeder machen, was er will?



.center[
.content-box-blue[Es doch den einen, richtigen, objektiven Priori-Wert geben?!]
]

</br>

.center[
.content-box-blue[Kann denn jeder hier machen, was er will?! Wo kommen wir da hin?!]
]

</br>
</br>

>    This is a mistake. There is no more a uniquely correct prior than there is a uniquely correct likelihood. Statistical models are machines for inference. Many machines will work, but some work better than others. Priors can be wrong, but only in the same sense that a kind of hammer can be wrong for building a table.  `r RefManageR::Citet(bib, "mcelreath_statistical_2020")`, p. 96.



---


## Hier ist unser Modell, `m43a`


.pull-left[

$$
\begin{align}
\text{height}_i &\sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta \cdot  \text{weight}_i\\
\alpha &\sim \operatorname{Normal}(178, 20)\\
\beta &\sim \operatorname{Normal}(5,3)\\
\sigma &\sim \operatorname{Exp}(0.1)
\end{align}
$$

]

.pull-right[

```{r Post-Regression-25, echo = TRUE}
# Zufallszahlen festlegen:
set.seed(42)  
# Posteriori-Vert. berechnen:
m43a <-
  stan_glm(height ~ weight_c, 
           prior = normal(0, 10),
           prior_intercept = normal(178, 20),  # mu
           prior_aux = exponential(0.1),  # sigma
           refresh = FALSE,
           data = d2)
```


]

```{r Post-Regression-26, echo = TRUE}
print(m43a,
        pars = c("(Intercept)", "weight_c"))
```


Voilà! Die Posteriori-Verteilung für `m43a`.

---
